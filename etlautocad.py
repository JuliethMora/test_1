# -*- coding: utf-8 -*-
"""etlautocad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mM7YK0k-S2gylL9KWlUvCy666BraaBwL

# INICIO
"""

import pandas as pd
import sys

# Wrap pandas.read_excel to provide a clear message if openpyxl is missing.
_orig_read_excel = pd.read_excel
def _safe_read_excel(*args, **kwargs):
    try:
        return _orig_read_excel(*args, **kwargs)
    except ImportError as e:
        msg = str(e)
        if 'openpyxl' in msg or "Missing optional dependency 'openpyxl'" in msg:
            print("❌ Error al leer archivo: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.")
            sys.exit(1)
        raise

pd.read_excel = _safe_read_excel
import numpy as np
import os
#from google.colab import drive
#drive.mount('/content/drive')
# import libs
#os.chdir('/content/drive/My Drive/MOVILIDAD')
#from headers import file_headers


#os.chdir('/content/drive/My Drive/proyecto_mov')

#Read excel file

#dataset = 'MV_05_052_1744_10_C2.xlsx'
#sheet_name = "VERTICAL"

#Ingresar el excel principal del proyecto

while True:
    dataset = input("Ingresa nombre del archivo de excel del proyecto: ").strip()
    if os.path.exists(dataset):
        print("nombre válido:", dataset)
        break
    else:
        print("archivo no encontrado. Por favor, intenta de nuevo.")

def sheet_name_file(path):
    xls = pd.read_excel(path,sheet_name= None)
    return print("sheet names found:",list(xls.keys()))

try:
    sheet_file_excel = sheet_name_file(dataset)
except:
    print('No se encontro archivo')

import os
import pandas as pd
import re


def file_headers(file_path):
    # Esta función transforma un valor individual
    def transformar(valor):
        # Extrae los números usando regex
        numeros = re.findall(r'\d+', str(valor))
        if len(numeros) == 2:
            return f'CTO {numeros[0]} DE {numeros[1]}'
        return valor  # Si no coincide, devuelve el valor original

    def transform(valor):
        if isinstance(valor, str):
            # Caso con guion exacto formato años
            m = re.match(r'^\s*(\d{4})-(\d{4})\s*$', valor)
            if m:
                return f"CTO {m.group(1)} DE {m.group(2)}"
            # Caso sin guion pero con dos tokens
            tokens = valor.strip().split()
            if len(tokens) >= 2:
                return f"{tokens[0]} DE {tokens[1]}"
        return valor

    def read_and_transform(sheet_name, header_rows):
        df = pd.read_excel(file_path, sheet_name=sheet_name, header=None, nrows=5)
        del df[2]
        df = df.iloc[1:, 1:4].T.dropna()
        df_raw = (
            df.copy()
            .rename(columns=df.iloc[0])
            .drop(index=df.index[0])
            .reset_index(drop=True)
        )
        df_raw['FECHA_INICIAL_CON'] = (
            pd.to_datetime(df_raw['CONTRATO'].str.slice(-4), errors='coerce')
            .dt.strftime('%d/%m/%Y')
        )
        df_raw = df_raw.rename(columns={
            'CONTRATO': 'NUMERO_CONTRATO',
            'NOMBRE PLANO': 'INTERNO_DISENO'
        })

        # Aplicar transformación al número de contrato
        df_raw['NUMERO_CONTRATO'] = df_raw['NUMERO_CONTRATO'].apply(transformar)

        df_raw_1 = df_raw[['NUMERO_CONTRATO', 'FECHA_INICIAL_CON']].copy()

        return df_raw_1, df_raw

    try:
        df1, df_full = read_and_transform('VERTICAL', header_rows=None)

        if df_full.empty:
            print("⚠️ 'VERTICAL' está vacío. Intentando con la hoja 'DEMARCACION'...")
            df1, df_full = read_and_transform('DEMARCACION', header_rows=None)

    except Exception as e:
        print("❌ Error al leer archivo:", e)
        return pd.DataFrame(), pd.DataFrame()

    return df1, df_full
df_raw_cols, df_raw = file_headers(dataset)

if df_raw_cols.empty and df_raw.empty:
    print("❗ No se cargó ninguna hoja válida.")
else:
    print("✅ Datos cargados desde hoja:",
          "DEMARCACION" if df_raw_cols.empty else "VERTICAL")
    #display(df_raw_cols.head(), df_raw.head())

"""## Tabla 1. CON_CONTRATO"""

# Crear la tabla con columnas definidas
create_table_sql = """
CREATE TABLE CON_CONTRATO(
    NUMERO_CONTRATO TEXT,
    FECHA_INICIAL_CON DATETIME
)
"""

try:
    cursor.execute(create_table_sql)
    conn.commit()
    print("✅ Tabla 'CON_CONTRATO' creada correctamente.")
except Exception as e:
    print("⚠️ La tabla ya existe o se produjo un error:", e)

try:
  # Lista de columnas para insertar
  columnas = ['NUMERO_CONTRATO', 'FECHA_INICIAL_CON']

  # SQL de inserción
  insert_sql = f"""
  INSERT INTO CON_CONTRATO (
      {', '.join(f'[{col}]' for col in columnas)}
  ) VALUES ({', '.join(['?'] * len(columnas))})
  """

  # Insertar datos desde el DataFrame
  for _, row in df_raw_cols.iterrows():
      values = [row.get(col, None) for col in columnas]
      try:
          cursor.execute(insert_sql, values)
      except Exception as e:
          print(f"⚠️ Error inserting row: {values}. Error: {e}")

  # Finalizar conexión
  conn.commit()
  cursor.close()
  conn.close()
  print("✅ Datos insertados exitosamente.")
except Exception as e:
  print("⚠️ no se han  insertados datos:", e)

"""## TABLA 2. PRO_PROYECTO"""

#path = 'c:/Users/Julieth Mora/Downloads/'
file_1 = 'INTERNO_PROYECTO.xlsx'
sheet_name = "sdm"
#os.chdir(path)
dataset_1 = file_1
df_interno = pd.read_excel(dataset_1)

df_interno

df_pro_1 = df_raw.merge(df_interno, how= 'inner', on ='INTERNO_DISENO')
df_pro_1

df_raw['INTERNO_DISENO']

df_interno['INTERNO_DISENO']

try:
  if df_pro_1.empty:
     df_raw['INTERNO_DISENO'] = df_raw['INTERNO_DISENO'].str[:-3]

     df_pro_1 = df_raw.merge(df_interno, how= 'inner', on ='INTERNO_DISENO')

     print(df_pro_1)
except Exception as e:
    print(f"An error occurred: {e}")

df_pro_1

#cursor.execute("DROP TABLE [PRO_PROYECTO]")

create_table_sql = """
CREATE TABLE PRO_PROYECTO (
    INTERNO_PROYECTO TEXT(255),
    ENTIDAD TEXT(255),
    INTERNO_DISENO TEXT(255),
    FECHA_REGISTRO DATETIME,
    INTERNO_CARACTERISTICA TEXT(255),
    ESTADO_PROYECTO TEXT(255),
    TIPO TEXT(255),
    OBJECTID INT
)
"""


try:
    cursor.execute(create_table_sql)
    conn.commit()
    print("✅ Tabla 'PRO_PROYECTO' creada correctamente.")
except Exception as e:
    print("⚠️ La tabla ya existe o se produjo un error:", e)

df_pro_1.columns

from datetime import date
df_pro_1['TIPO'] = 'RECORD'
df_pro_1['FECHA_REGISTRO'] = date.today()
table_2 = df_pro_1[['INTERNO_PROYECTO','ENTIDAD','INTERNO_DISENO','FECHA_REGISTRO','INTERNO_CARACTERISTICA','ESTADO_PROYECTO','TIPO']]

try:
  # Lista de columnas para insertar
  columnas = ['INTERNO_PROYECTO','ENTIDAD','INTERNO_DISENO','FECHA_REGISTRO','ESTADO_PROYECTO','TIPO']

  # SQL de inserción
  insert_sql = f"""
  INSERT INTO PRO_PROYECTO (
      {', '.join(f'[{col}]' for col in columnas)}
  ) VALUES ({', '.join(['?'] * len(columnas))})
  """

  # Insertar datos desde el DataFrame
  for _, row in df_pro.iterrows():
      values = [row.get(col, None) for col in columnas]
      try:
          cursor.execute(insert_sql, values)
      except Exception as e:
          print(f"⚠️ Error inserting row: {values}. Error: {e}")

  # Finalizar conexión
  conn.commit()
  cursor.close()
  conn.close()
  print("✅ se han  insertados datos.")
except Exception as e:
  print("⚠️ no se han  insertados datos.", e)

table_2['TIPO']= 'RECORD'

table_2.info()

table_2['FECHA_REGISTRO'] = pd.to_datetime(
    table_2['FECHA_REGISTRO'],dayfirst=True
)

table_2

table_2['FECHA_REGISTRO'] = table_2[
    'FECHA_REGISTRO'
].dt.strftime('%d/%m/%Y')

#table_2.to_excel(f'PRO_PROYECTO_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""## TABLE 3. PRO_PROYECTO_LOCALIDAD"""

#Create df localidad

data = {
    "CODIGO_LOCALIDAD": [
        0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
        10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20
    ],
    "LOCALIDAD": [
        "NO APLICA", "USAQUÉN", "CHAPINERO", "SANTA FE",
        "SAN CRISTOBAL", "USME", "TUNJUELITO", "BOSA",
        "KENNEDY", "FONTIBON", "ENGATIVÁ", "SUBA",
        "BARRIOS UNIDOS", "TEUSAQUILLO", "LOS MARTIRES",
        "ANTONIO NARIÑO", "PUENTE ARANDA", "LA CANDELARIA",
        "RAFAEL URIBE", "CIUDAD BOLÍVAR", "SUMAPAZ"
    ]
}

df_localidades = pd.DataFrame(data)

df_localidades

df_pro_1['LOCALIDAD']

# Convertir lista de localidades en patrones regex escapados
localidades_list = df_localidades["LOCALIDAD"].tolist()
pattern = r"\b(" + "|".join(re.escape(loc) for loc in localidades_list) + r")\b"

def extraer_localidades(texto):
    if not isinstance(texto, str):
        return ""
    matches = re.findall(pattern, texto, flags=re.IGNORECASE)
    # Devolver coincidencias únicas y correctamente capitalizadas
    unique = []
    for m in matches:
        up = next((loc for loc in localidades_list if loc.lower() == m.lower()), m.upper())
        if up not in unique:
            unique.append(up)
    return ", ".join(unique)

# Aplicarlo al DataFrame
df_pro_1["LOCALIDAD_EXTRAIDA"] = df_pro_1["LOCALIDAD"].apply(extraer_localidades)

df_pro_1

del df_pro_1['LOCALIDAD']

df_pro_1 = df_pro_1.rename(columns={'LOCALIDAD_EXTRAIDA':'LOCALIDAD'})

df_raw = df_pro_1.merge(df_localidades, how='inner', on= 'LOCALIDAD' )

df_raw = df_raw.rename(columns={'INTERNO_PROYECTO':'INTERNO'})

df_raw_local = df_raw[['INTERNO','CODIGO_LOCALIDAD']]

df_raw_local

#cursor.execute("DROP TABLE [PRO_PROYECTO_LOCALIDAD]")

# Crear la tabla con columnas definidas
create_table_sql = """
CREATE TABLE PRO_PROYECTO_LOCALIDAD(
    INTERNO INTEGER,
    CODIGO_LOCALIDAD INTEGER
)
"""

try:
    cursor.execute(create_table_sql)
    conn.commit()
    print("✅ Tabla 'PRO_PROYECTO_LOCALIDAD' creada correctamente.")
except Exception as e:
    print("⚠️ La tabla ya existe o se produjo un error:", e)

try:
  # Lista de columnas para insertar
  columnas = ['INTERNO','CODIGO_LOCALIDAD']
  # SQL de inserción
  insert_sql = f"""
  INSERT INTO PRO_PROYECTO_LOCALIDAD (
      {', '.join(f'[{col}]' for col in columnas)}
  ) VALUES ({', '.join(['?'] * len(columnas))})
  """

  # Insertar datos desde el DataFrame
  for _, row in df_raw.iterrows():
      values = [row.get(col, None) for col in columnas]
      try:
          cursor.execute(insert_sql, values)
      except Exception as e:
          print(f"⚠️ Error inserting row: {values}. Error: {e}")

  # Finalizar conexión
  conn.commit()
  cursor.close()
  conn.close()
  print("✅ Datos insertados exitosamente.")
except Exception as e:
  print(" no se han  insertados Datos exitosamente.", e)

df_raw_local.to_excel(f'PRO_PROYECTO_LOCALIDAD_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""## TABLE 4  PRO_DETALLE_PROYECTO

"""

#cursor.execute("DROP TABLE [PRO_PRO]")

create_table_sql = """
CREATE TABLE PRO_DETALLE_PROYECTO(
    INTERNO_PROYECTO INT,
    PRIORIDAD_DISENO TEXT(255),
    TIPO_DETALLE TEXT,
    UNIDAD_DIBUJO TEXT(255),
    VALOR_UNIDAD DOUBLE,
    TIPO_DIBUJO TEXT(255)
)
"""

try:
    cursor.execute(create_table_sql)
    conn.commit()
    print("✅ Tabla 'PRO_DETALLE_PROYECTO' creada correctamente.")
except Exception as e:
    print("⚠️ La tabla ya existe o se produjo un error:", e)

import numpy as np

def type_det(df):
    # Extraemos los dos primeros caracteres
    df['TIPO_DETALLE_1'] = df['INTERNO_DISENO'].str.slice(0, 2)

    # Definimos condiciones
    conds = [
        df['TIPO_DETALLE_1'] == 'MV',
        df['TIPO_DETALLE_1'] == 'EX'
    ]
    choices = [
        'CORREDOR VIAL',
        'EXPANSION'
    ]

    # Asignamos el valor según condición, por defecto: 'CORREDOR VIAL'
    df['TIPO_DETALLE'] = np.select(conds, choices, default='CORREDOR VIAL')
    del df['TIPO_DETALLE_1']
    return df
df_raw = type_det(df_raw)

df_raw['PRIODIDAD_DISENO'] = 'NORMAL'
df_raw['TIPO_DIBUJO'] = 'OTRO'
df_raw = df_raw.rename(columns= {'INTERNO':'INTERNO_PROYECTO'})

df_raw['PRIORIDAD_DISENO'] = 'NORMAL'

df_raw.columns

df_raw['UNIDAD_DIBUJO'] = 'UNIDAD'
df_raw['VALOR_UNIDAD'] = None

try:
  # Lista de columnas para insertar
  columnas = ['INTERNO_PROYECTO','PRIORIDAD_DISENO','TIPO_DETALLE','UNIDAD_DIBUJO', 'VALOR_UNIDAD','TIPO_DIBUJO']
  # SQL de inserción
  insert_sql = f"""
  INSERT INTO PRO_DETALLE_PROYECTO(
      {', '.join(f'[{col}]' for col in columnas)}
  ) VALUES ({', '.join(['?'] * len(columnas))})
  """

  # Insertar datos desde el DataFrame
  for _, row in df_raw.iterrows():
      values = [row.get(col, None) for col in columnas]
      try:
          cursor.execute(insert_sql, values)
      except Exception as e:
          print(f"⚠️ Error inserting row: {values}. Error: {e}")


  # Finalizar conexión
  conn.commit()
  cursor.close()
  conn.close()
  print("✅ Datos insertados exitosamente.")
except Exception as e:
  print("⚠️ no se han  insertados datos.", e)

table_3 = df_raw[['INTERNO_PROYECTO','PRIORIDAD_DISENO','TIPO_DETALLE','UNIDAD_DIBUJO', 'VALOR_UNIDAD','TIPO_DIBUJO']]
#table_3.to_excel(f'PRO_DETALLE_PROYECTO_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx', index = False)

"""# Export excel file CON CONTRATO Y PRO"""

output_filename_con_pro = f'Output_CONTRATO_PRO{table_2["INTERNO_DISENO"].iloc[0]}.xlsx'

with pd.ExcelWriter(output_filename_con_pro) as writer:
    df_raw_cols.to_excel(writer, sheet_name='CON_CONTRATO', index=False)
    table_2.to_excel(writer, sheet_name='PRO_PROYECTO', index=False)
    df_raw_local.to_excel(writer, sheet_name='PRO_PROYECTO_LOCALIDAD', index=False)
    table_3.to_excel(writer, sheet_name='PRO_DETALLE_PROYECTO', index=False)

print(f"✅ All dataframes exported to {output_filename_con_pro}")



"""## Table 5 GEOSEGMENTO"""

#Fuction read principal sheet proyect

def clean_multilevel_columns_v(file_path):
    df = pd.read_excel(file_path,
                       sheet_name="VERTICAL",
                       header=[6, 7],
                       )

    # Aplana el MultiIndex en tuplas
    flat = df.columns.to_flat_index()
    # Une los niveles omitiendo partes vacías o "Unnamed"
    cleaned = [
        '_'.join([str(v).strip() for v in tup if v and not str(v).startswith('Unnamed')])
        for tup in flat
    ]
    df.columns = cleaned
    if 'N°' in df.columns:
        df = df[pd.to_numeric(df['N°'], errors='coerce').notna()]
    else:
        print("⚠️ Atención: no se encontró la columna 'N°' tras limpieza.")

    return df

#read sheet proyect vertical
data = clean_multilevel_columns_v(dataset)

if data.empty:
    print("❗ DataFrame 'data' está vacío. Por favor, dirígete a la seccion DEMARCACION' para cargar un nuevo dataset.")

data = data.rename(columns={'N°':'ID', 'INTERNO':'INTERNO_SENAL','TIPO_ SEÑAL':'TIPO_SENAL', 'CLASE _SEÑAL':'CLASE_SENAL' })

data

data.columns = data.columns.str.strip().str.replace(" ", "_")
data.columns = data.columns.str.strip().str.replace(".", "_")
data.columns = data.columns.str.strip().str.replace("/", "_")
data.columns = data.columns.str.strip().str.replace("°", "_")
data.columns = data.columns.str.strip().str.replace("\n", "_")
data.columns = data.columns.str.strip().str.replace("Ñ", "N")
data.columns = data.columns.str.strip().str.replace("__", "_")

data.columns

data['ITEM_SUMINISTRO_INTERNO'].value_counts()

data

create_table_sql = """
CREATE TABLE GEO_SEGMENTO (
    CIV INT,
    ANCHO_CALZADA DOUBLE,
    NUMERO_CALZADA INT,
    NUMERO_CARRIL INT,
    SENTIDO_VIAL TEXT(50),
    LONGITUD_SEGMENTO DOUBLE,
    TIPO_SUPERFICIE TEXT(50),
    ESTADO_SUPERFICIE TEXT(50),
    NOMBRE_EJE TEXT(50),
    NOMBRE_EXTREMO_INICIO TEXT(50),
    NOMBRE_EXTREMO_FIN TEXT(50),
    FECHA_INTERVENCION DATE,
    FECHA_REGISTRO DATE
)
"""

try:
    cursor.execute(create_table_sql)
    conn.commit()
    print("✅ Tabla 'GEO_SEGMENTO' creada correctamente.")
except Exception as e:
    print("⚠️ La tabla ya existe o se produjo un error:", e)

data_geo = data.copy()

data_geo

data_geo.drop_duplicates(subset=['CIV'], keep='first', inplace=True)

try:
  #DIRECCION_EJE	DIRECCION_EJE_1	DIRECCION_INICIO	DIRECCION_INICIO_1
  data_geo['NOMBRE_EJE'] = data_geo['DIRECCION_EJE'].str.cat(data_geo['DIRECCION_EJE_1'], sep=' ')
  data_geo['NOMBRE_EXTREMO_INICIO'] = data_geo['DIRECCION_INICIO'].str.cat(data_geo['DIRECCION_INICIO_1'])
  data_geo['NOMBRE_EXTREMO_FIN']  = data_geo['DIRECCION_TERMINA'].str.cat(data_geo['DIRECCION_TERMINA_1'])
except:
  print(np.empty)

"""data_geo['ANCHO_CALZADA'] = # sE IDENTIFICAN DE SEPARADOR A SARDINEL  Y SE TOMA LA MEDIDA
data_geo['NUMERO_CALZADA'] = 1 # MISMO CIV 1  SI ES DIFERENTE SERIAN 2  CASI SIEMPRE ES 1
data_geo['NUMERO_CARRIL'] = 1 # TOMANDO ANCHO DE CALZADA NORMALMENTE MIDE UN CARRIL DE 3 A 4 MTROS CUANDO ES MAS DE 3 O CUANTRO SERIA 2
data_geo['SENTIDO_VIAL'] =  # IDENTIFICAR EN TABLA GEOSEGMENTO MVIETIQUET AHI ESTA LA DIRECCIÓN, SI LA DIRECCION ES CL O DG : SERIA ORIENTE-OCCIDENTE Ó OCCIDENTE ORIENTE : PARA TENER LA PRECISION
 #SE VALIDAN LOS civ que intersectan en el campo MIVetiquet teniendo en cuenta si va la nomenclatura  que intersecta de mayor a menor
"""



try:
  columns_geo = ["CIV", "ANCHO_CALZADA", "NUMERO_CALZADA", "NUMERO_CARRIL", "SENTIDO_VIAL",
                 "LONGITUD_SEGMENTO", "TIPO_SUPERFICIE", "ESTADO_SUPERFICIE", "NOMBRE_EJE", "NOMBRE_EXTREMO_INICIO", "NOMBRE_EXTREMO_FIN",
                 "FECHA_INTERVENCION", "FECHA_REGISTRO"]
  data_geo = data_geo[columns_geo]

except:
  print('No se ha generado, revisar')

try:
  # Lista de columnas para insertar
  columnas = ['CIV']
  # SQL de inserción
  insert_sql = f"""
  INSERT INTO GEO_SEGMENTO(
      {', '.join(f'[{col}]' for col in columnas)}
  ) VALUES ({', '.join(['?'] * len(columnas))})
  """

  # Insertar datos desde el DataFrame
  for _, row in data_geo.iterrows():
      values = [row.get(col, None) for col in columnas]
      try:
          cursor.execute(insert_sql, values)
      except Exception as e:
          print(f"⚠️ Error inserting row: {values}. Error: {e}")


  # Finalizar conexión
  conn.commit()
  cursor.close()
  conn.close()
  print("✅ Datos insertados exitosamente.")
except Exception as e:
  print("⚠️ no se han  insertados datos.", e)

"""## TABLE SEN_SENALIZACION"""

#cursor.execute("DROP TABLE [SEN_SENALIZACION]")

data.columns

data = data.rename(columns={'N°':'ID', 'INTERNO':'INTERNO_SENAL','TIPO_ SEÑAL':'TIPO_SENAL', 'CLASE _SEÑAL':'CLASE_SENAL' })

data

create_table_sql= """
CREATE TABLE SEN_SENALIZACION(
    ID INT,
    INTERNO_SENAL TEXT(255),
    TIPO_PEDESTAL TEXT(255),
    CIV INT,
    TIPO_SENAL TEXT(255),
    CLASE_SENAL TEXT(255),
    DIRECCION TEXT(255)
)
"""

try:
    cursor.execute(create_table_sql)
    conn.commit()
    print("✅ Tabla 'SEN_SENALIZACION' creada correctamente.")
except Exception as e:
    print("⚠️ La tabla ya existe o se produjo un error:", e)

try:
  # Lista de columnas para insertar
  columnas = ['ID','INTERNO_SENAL','TIPO_PEDESTAL','CIV','TIPO_SENAL','CLASE_SENAL','DIRECCION']
  # SQL de inserción
  insert_sql = f"""
  INSERT INTO SEN_SENALIZACION (
      {', '.join(f'[{col}]' for col in columnas)}
  ) VALUES ({', '.join(['?'] * len(columnas))})
  """

  # Insertar datos desde el DataFrame
  for _, row in data.iterrows():
      values = [row.get(col, None) for col in columnas]
      try:
          cursor.execute(insert_sql, values)
      except Exception as e:
          print(f"⚠️ Error inserting row: {values}. Error: {e}")

  # Finalizar conexión
  conn.commit()
  cursor.close()
  conn.close()
  print("✅ Datos insertados exitosamente.")
except Exception as e:
  print("⚠️ no se han  insertados datos.", e)

def build_address(row):
    # Convertir a string solo si no es NaN
    eje_1 = str(row['DIRECCION_EJE']) if pd.notna(row['DIRECCION_EJE']) else ''
    eje_2 = str(row['DIRECCION_EJE_1']) if pd.notna(row['DIRECCION_EJE_1']) else ''
    inicio_1 = str(row['DIRECCION_INICIO']) if pd.notna(row['DIRECCION_INICIO']) else ''
    inicio_2 = str(row['DIRECCION_INICIO_1']) if pd.notna(row['DIRECCION_INICIO_1']) else ''
    termina_1 = str(row['DIRECCION_TERMINA']) if pd.notna(row['DIRECCION_TERMINA']) else ''
    termina_2 = str(row['DIRECCION_TERMINA_1']) if pd.notna(row['DIRECCION_TERMINA_1']) else ''

    parte_eje = f"{eje_1} {eje_2}".strip()
    parte_inicio = f"{inicio_1} {inicio_2}".strip()
    parte_termina = f"{termina_1} {termina_2}".strip()

    if not parte_inicio and not parte_termina:
        return parte_eje
    else:
        return f"{parte_eje} DESDE {parte_inicio} HASTA {parte_termina}".strip()

# Crear una máscara booleana donde al menos dos columnas tienen el mismo valor
mask_minimo_dos_iguales = data.apply(
    lambda row: (
        row['DIRECCION_EJE_1'] == row['DIRECCION_INICIO_1'] or
        row['DIRECCION_EJE_1'] == row['DIRECCION_TERMINA_1'] or
        row['DIRECCION_INICIO_1'] == row['DIRECCION_TERMINA_1']
    ),
    axis=1
)

# Filtrar el DataFrame usando la máscara
df_duplicate_tramos_sen = data.loc[mask_minimo_dos_iguales].copy()
df_duplicate_tramos_sen

data_sen = data.copy()
data_sen['DIRECCION'] = data.apply(build_address, axis=1)

data_sen = data_sen[['ID','INTERNO_SENAL','TIPO_PEDESTAL','CIV','TIPO_SENAL','CLASE_SENAL','DIRECCION']]
#data_sen.to_excel(f'SEN_SENALIZACION_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""# **SEN_DETALLE_SENALIZACION**


"""

import pandas as pd

def clean_multilevel_columns_v(file_path):
    # --- leer y aplanar encabezados ---
    df = pd.read_excel(file_path, sheet_name="VERTICAL", header=[6, 7])
    flat = df.columns.to_flat_index()
    df.columns = [
        "_".join([str(v).strip() for v in tup if v and not str(v).startswith("Unnamed")])
        for tup in flat
    ]

    # --- normalización de nulos y espacios en columnas de texto ---
    obj_cols = df.select_dtypes(include="object").columns
    NULL_TOKENS = {
        "", " ", "-", "—", "N/A", "NA", "n/a", "na",
        "None", "NULL", "null", "Nan", "NaN", "nan", "S/D", "SD", "N/D"
    }
    if len(obj_cols):
        df[obj_cols] = df[obj_cols].apply(
            lambda s: s.apply(lambda x: pd.NA if (isinstance(x, str) and x.strip() in NULL_TOKENS) else x)
        )

    # --- N° numérico (True = válido) ---
    if "N°" in df.columns:
        n_numeric = pd.to_numeric(df["N°"], errors="coerce").notna()
    else:
        n_numeric = pd.Series(False, index=df.index)

    # --- máscaras de datos en campos clave ---
    # Dimensiones a numérico; 0 o negativos -> NA
    def _num_pos(series):
        if series is None:  # columna ausente
            return pd.Series([pd.NA]*len(df), index=df.index)
        s = pd.to_numeric(series, errors="coerce")
        s = s.where(s > 0)  # 0/negativos como vacío
        return s

    ancho = _num_pos(df["DIMENSIONES_ANCHO"]) if "DIMENSIONES_ANCHO" in df.columns else None
    alto  = _num_pos(df["DIMENSIONES_ALTO"])  if "DIMENSIONES_ALTO"  in df.columns else None

    dims_con_dato = (
        (ancho.notna() if ancho is not None else pd.Series(False, index=df.index)) |
        (alto.notna()  if alto  is not None else pd.Series(False, index=df.index))
    )

    if "CONTENIDO" in df.columns:
        contenido_con_dato = df["CONTENIDO"].astype(str).str.strip()
        contenido_con_dato = contenido_con_dato.ne("") & df["CONTENIDO"].notna()
    else:
        contenido_con_dato = pd.Series(False, index=df.index)

    campos_con_dato = dims_con_dato | contenido_con_dato

    # --- regla de filtrado: drop si N° NO numérico y campos vacíos ---
    keep_mask = n_numeric | campos_con_dato
    df = df[keep_mask].copy().reset_index(drop=True)

    # (opcional) escribir de vuelta dimensiones ya limpiadas
    if "DIMENSIONES_ANCHO" in df.columns and ancho is not None:
        df["DIMENSIONES_ANCHO"] = pd.to_numeric(df["DIMENSIONES_ANCHO"], errors="coerce").where(
            pd.to_numeric(df["DIMENSIONES_ANCHO"], errors="coerce") > 0
        )
    if "DIMENSIONES_ALTO" in df.columns and alto is not None:
        df["DIMENSIONES_ALTO"] = pd.to_numeric(df["DIMENSIONES_ALTO"], errors="coerce").where(
            pd.to_numeric(df["DIMENSIONES_ALTO"], errors="coerce") > 0
        )

    # eliminar filas completamente vacías tras limpieza (seguro)
    df = df.dropna(how="all").reset_index(drop=True)

    return df

#read sheet proyect vertical
data = clean_multilevel_columns_v(dataset)

if data.empty:
    print("❗ DataFrame 'data' está vacío. Por favor, dirígete a la seccion DEMARCACION' para cargar un nuevo dataset.")

data = data.rename(columns={'N°':'ID', 'INTERNO':'INTERNO_SENAL','TIPO_ SEÑAL':'TIPO_SENAL', 'CLASE _SEÑAL':'CLASE_SENAL' })

data.columns = data.columns.str.strip().str.replace(" ", "_")
data.columns = data.columns.str.strip().str.replace(".", "_")
data.columns = data.columns.str.strip().str.replace("/", "_")
data.columns = data.columns.str.strip().str.replace("°", "_")
data.columns = data.columns.str.strip().str.replace("\n", "_")
data.columns = data.columns.str.strip().str.replace("Ñ", "N")
data.columns = data.columns.str.strip().str.replace("__", "_")


import pandas as pd
import numpy as np


def duplicar_senal_duplex(df):
    """
    Si:
      - df['CLASE_SENAL'] == 'SENAL DUPLEX'
      - o df['TIPO_SENAL'] contiene 'PLAQUETA'
    y la siguiente fila tiene N° vacío o nulo,
    copia los valores de la fila actual a la siguiente excepto en
    ['DIMENSIONES_ANCHO', 'DIMENSIONES_ALTO', 'CONTENIDO'] que conservan lo que ya tienen.
    """
    campos_excluir = ['DIMENSIONES_ANCHO', 'DIMENSIONES_ALTO', 'CONTENIDO']

    for i in df.index:
        condicion = (
            (df.loc[i, 'CLASE_SENAL'] == 'SENAL DUPLEX') or
            (isinstance(df.loc[i, 'TIPO_SENAL'], str) and 'PLAQUETA' in df.loc[i, 'TIPO_SENAL'].upper())
        )

        if condicion and i + 1 in df.index and (pd.isna(df.loc[i+1, 'ID']) or df.loc[i+1, 'ID'] == ''):
            for col in df.columns:
                if col not in campos_excluir:
                    df.loc[i+1, col] = df.loc[i, col]

    return df


#data = duplicar_senal_duplex(data)

df = data#duplicar_senal_duplex(data)
df

import pandas as pd

def dividir_y_bajar(df):
    """
    Para cada fila con '/' en TIPO_SENAL:
    - La fila actual se queda con la parte antes del '/'
    - La fila siguiente (si existe) recibe:
        * La parte después del '/' en TIPO_SENAL
        * Copia de INTERNO_SEÑAL de la fila actual
    """
    df = df.copy()  # trabajar sobre copia para no modificar original

    for idx in range(len(df)-1):  # hasta la penúltima fila
        valor = df.loc[idx, 'TIPO_SENAL']
        if isinstance(valor, str) and '/' in valor:
            parte1, parte2 = valor.split('/', 1)
            # Actualiza fila actual
            df.loc[idx, 'TIPO_SENAL'] = parte1.strip()
            # Actualiza fila siguiente
            df.loc[idx+1, 'TIPO_SENAL'] = parte2.strip()
            df.loc[idx+1, 'INTERNO_SENAL'] = df.loc[idx, 'INTERNO_SENAL']

    return df

# Uso:
df = dividir_y_bajar(df)

df

"""
def dividir_tipo_senal_plaqueta(df):
    """
"""    Para filas con TIPO_SENAL que contienen la palabra 'PLAQUETA' y con '/' en TIPO_SENAL:
    - La primera aparición conserva la parte antes del '/'
    - La segunda aparición conserva la parte después del '/'
    """
"""
  # Filtrar filas donde TIPO_SENAL contiene "PLAQUETA"
    plaqueta_idx = df[df['TIPO_SENAL'].str.contains("PLAQUETA", case=False, na=False)].index

    for i, idx in enumerate(plaqueta_idx):
        valor = df.loc[idx, 'TIPO_SENAL']
        if isinstance(valor, str) and "/" in valor:
            parte1, parte2 = valor.split("/", 1)
            if i % 2 == 0:  # primera aparición → antes del "/"
                df.loc[idx, 'TIPO_SENAL'] = parte1.strip()
            else:           # segunda aparición → después del "/"
                df.loc[idx, 'TIPO_SENAL'] = parte2.strip()
    return df


# Ejecución con control de error
try:
    data = dividir_tipo_senal_plaqueta(data)
except Exception as e:
    print("No hay registros con PLAQUETA:", e)
"""

df.columns

#df['TIPO_SENAL'].str.contains('/', na=False)

#df[df['TIPO_SENAL'].str.contains("/", case=False, na=False)].index

df

"""
def dividir_tipo_senal_existente(df):
"""
"""
    Para filas con CLASE_SENAL == 'SENAL DUPLEX' y con '/' en TIPO_SENAL:
    - La primera aparición conserva la parte antes del '/'
    - La segunda aparición conserva la parte después del '/'
    """
"""    # Filtrar solo las filas duplex
    duplex_idx =df[df['TIPO_SENAL'].str.contains("/", case=False, na=False)].index #df['TIPO_SENAL'].str.contains('/', na=False)#df[df['CLASE_SENAL'] == 'SENAL DUPLEX'].index

    for i, idx in enumerate(duplex_idx):
        valor = df.loc[idx, 'TIPO_SENAL']
        if isinstance(valor, str) and "/" in valor:
            parte1, parte2 = valor.split("/", 1)
            if i % 2 == 0:  # primera aparición → antes del "/"
                df.loc[idx, 'TIPO_SENAL'] = parte1.strip()
            else:           # segunda aparición → después del "/"
                df.loc[idx, 'TIPO_SENAL'] = parte2.strip()
    return df
try:
  data = dividir_tipo_senal_existente(df)
except:
  print('no hay senal duplex') """

try:
  table_6 = data.copy()
  table_6['NUMERO_CONTRATO'] = df_pro_1['NUMERO_CONTRATO'].iloc[0]
  table_6['FECHA_REGISTRO'] = df_pro_1['FECHA_REGISTRO'].iloc[0]
  table_6['INTERNO_DETALLE'] = range(1, len(table_6) + 1)

  table_6 = table_6[["INTERNO_DETALLE","ID","INTERNO_SENAL","NUMERO_CONTRATO","FASE","ACCION","ESTADO","FECHA_FASE","OBSERVACIONES","NUMERO_CUENTA","FECHA_REGISTRO"
  ]]
except:
  print('No se ha generado, revisar')

try:
  table_6['FECHA_REGISTRO'] = pd.to_datetime(
      table_6['FECHA_REGISTRO'],dayfirst=True
  )
except:
  print('No se ha generado, revisar')

try:
  table_6['FECHA_REGISTRO'] = table_6[
      'FECHA_REGISTRO'
  ].dt.strftime('%d/%m/%Y')
except:
  print('No se ha generado, revisar')

try:
  table_6['FECHA_FASE'] = pd.to_datetime(
      table_6['FECHA_FASE'],dayfirst=True
  )
except:
  print('review')

try:
  table_6['FECHA_FASE'] = table_6[
      'FECHA_FASE'
  ].dt.strftime('%d/%m/%Y')
except:
  print('review')

table_6

table_6

""" try:
    # Inicializa la columna si no existe
    table_6['OBSERVACIONES'] = None

    # 1. Crea la máscara sobre el DF original
    mask_to_replace = (table_6['ACCION'] == 'REEMPLAZAR')

    # 2. Genera los dos DataFrames derivados sobre esa máscara
    df_include = table_6[mask_to_replace].copy()
    df_include['ACCION'] = 'INSTALACION'   # corrige ortografía si aplica
    df_include['ESTADO'] = 'BUENO'         # revisa si este estado es correcto

    df_include_1 = table_6[mask_to_replace].copy()
    df_include_1['ACCION'] = 'RETIRAR'
    df_include_1['ESTADO'] = 'MALO'

    # 3. Quita primero las filas originales 'REEMPLAZAR'
    table_6 = table_6[~mask_to_replace].copy()

    # 4. Concatena todos los DF
    table_6 = pd.concat([table_6, df_include, df_include_1],
                        ignore_index=True)

    # 5. Reinicia índice
    table_6.reset_index(drop=True, inplace=True)

except Exception as e:
    print('No se ha generado, revisar:', e) """

table_6.dropna(subset= ['ID'],inplace=True)

table_6.info()

table_6['INTERNO_DETALLE'] = range(1, len(table_6) + 1)

table_6

#table_6.to_excel(f'SEN_DETALLE_SENALIZACION_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

#table_6.to_excel(f'SEN_DETALLE_SENALIZACION_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""# SEN_DESCRIPCION_TABLERO"""

table_7 = df.copy()

table_7['INTERNO_TABLERO'] = range(1, len(table_7) + 1)

table_7 = table_7.rename(columns= { 'DIMENSIONES_ANCHO':'ANCHO',
       'DIMENSIONES_ALTO': 'ALTO'})

table_7.columns

table_7

try:

    table_7 = table_7[["INTERNO_TABLERO","ID","INTERNO_SENAL","TIPO_SENAL","CLASE_SENAL","TIPO_REFLECTIVO","MATERIAL_TABLERO","ANCHO","ALTO"]]

    # Create conditions and choices for mapping
    conditions = [
        table_7['TIPO_SENAL'].str.contains('SR', na=False),
        table_7['TIPO_SENAL'].str.contains('SP', na=False),
        table_7['TIPO_SENAL'].str.contains('SI', na=False),
        table_7['TIPO_SENAL'].str.contains('SIO', na=False),
        table_7['TIPO_SENAL'].str.contains('PLAQUETA', na=False)
    ]
    choices = [
        'SENAL REGLAMENTARIA',
        'SENAL PREVENTIVA',
        'SENAL INFORMATIVA',
        'SENAL INFORMATIVA DE OBRA',
        'NO APLICA'
    ]

    # Apply the mapping using numpy.select
    table_7['CLASE_SENAL'] = np.select(conditions, choices, default=table_7['CLASE_SENAL'])
except:
        print('No data')

sp_46_b = table_7[table_7['TIPO_SENAL']=='SP-46B']
sp_46_b

sp_47b = table_7[table_7['TIPO_SENAL']== 'SP-47B']
sp_47b



table_7_1 = pd.concat([table_7,sp_46_b,sp_47b],ignore_index=True)

table_7_1[table_7_1['TIPO_SENAL']== 'SP-46B']

table_7_1.dropna(inplace=True)

table_7_1.info()

table_7_1.isna().sum()
#SP-46B
#SP-47B

table_7_1

table_7

table_7 = table_7.replace('', pd.NA)
    # luego rellenamos con el valor anterior en todas las columnas
table_7 = table_7.ffill()

table_7.info()

table_7.isna().sum()

#table_7_1.to_excel(f'SEN_DESCRIPCION_TABLERO_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""# SEN_CONTENIDO_TABLERO"""

df

table_8 = df.copy()

table_8

table_8['CONTENIDO'].value_counts()

contenido_not_na =table_8[(table_8['CONTENIDO'].notna() | (table_8['CONTENIDO'] != 0))]

contenido_not_na

table_8['INTERNO_CONTENIDO'] = range(1, len(table_8) + 1)
table_8['INTERNO_TABLERO'] = range(1, len(table_8) +1)

# Filter rows where 'CONTENIDO' is not zero or null AND 'TIPO_FLECHA' is not zero or null
table_8 = table_8[(table_8['CONTENIDO'].notna() | (table_8['CONTENIDO'] != 0)
          |(table_8['TIPO_FLECHA'].notna())|
          (table_8['TIPO_FLECHA'] != 0))]

table_8[table_8['TIPO_FLECHA'].notna()| (table_8['TIPO_FLECHA'] != 0)]

# Llenar NaN o vacíos con el dato anterior en columnas seleccionadas
cols = ["INTERNO_CONTENIDO", "INTERNO_TABLERO", "INTERNO_SENAL", "TIPO_FLECHA"]

# Primero sustituimos strings vacíos por NaN para que ffill los tome
table_8[cols] = table_8[cols].replace(r'^\s*$', np.nan, regex=True)

# Luego hacemos forward fill
table_8[cols] = table_8[cols].fillna(method='ffill')

table_8

# Filtrar filas donde CONTENIDO no es nulo y es distinto de 0
table_8_filtrado = table_8[
    table_8['CONTENIDO'].notna() & (table_8['CONTENIDO'] != 0)
]

table_8_filtrado

table_8_filtrado= table_8_filtrado[["INTERNO_CONTENIDO","INTERNO_TABLERO","INTERNO_SENAL","TIPO_FLECHA","CONTENIDO"]]

table_8_filtrado
table_8_filtrado['TIPO_FLECHA'] = table_8_filtrado['TIPO_FLECHA'] .fillna('NO APLICA')
table_8_filtrado['CONTENIDO'] = table_8_filtrado['CONTENIDO'].fillna('NO APLICA')

#table_8['TIPO_FLECHA'] = table_8['TIPO_FLECHA'] .fillna('NO APLICA')
#table_8['CONTENIDO'] = table_8['CONTENIDO'].fillna('NO APLICA')

#table_8 = table_8[["INTERNO_CONTENIDO","INTERNO_TABLERO","INTERNO_SENAL","TIPO_FLECHA","CONTENIDO"
#]]

table_8_filtrado.info()

#['CLASE_SENAL']= 'SENAL DUPLEX' #QUE SE HA DUPLICADO  CLASE SEÑAL VA A SER DE CUERDO AL TIPO DE SENAL SR'--> SENAL REGLAMENTARIA... SI--> SENAL INFORMATIVA .. SP--> SENAL PREVENTIVA ... SRO ---> SENAL INFORMATIVA DE OBRA

table_8_filtrado.isna().sum()

#table_8.to_excel(f'SEN_CONTENIDO_TABLERO_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""# SEN_SEÑALIZACION_PROYECTO"""

data.columns

df_sen_project = table_6.copy()



df_sen_project['INTERNO_PROYECTO'] = df_pro_1['INTERNO_PROYECTO'].iloc[0]

df_sen_project = df_sen_project[["INTERNO_DETALLE","INTERNO_SENAL","INTERNO_PROYECTO"]]

df_sen_project.to_excel(f'SEN_SEÑALIZACION_PROYECTO_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""# SEN_ITEM"""

#Aquí se debe leer de nuevo la planilla, para evitar que se omitan valores, importante comparar antes de enviar.

import pandas as pd

def clean_multilevel_columns(file_path):
    # Leer Excel con encabezado multinivel
    df = pd.read_excel(
        file_path,
        sheet_name="VERTICAL",
        header=[6, 7]
    )

    # Aplana el MultiIndex
    flat = df.columns.to_flat_index()

    # Limpia nombres de columnas y crea nombres únicos
    seen = {}
    cleaned = []
    for tup in flat:
        parts = [str(v).strip() for v in tup if v and not str(v).startswith('Unnamed')]
        base_name = '_'.join(parts) if parts else 'COL'
        if base_name in seen:
            seen[base_name] += 1
            base_name = f"{base_name}_{seen[base_name]}"
        else:
            seen[base_name] = 1
        cleaned.append(base_name)

    df.columns = cleaned

    # Elimina filas completamente vacías
    df = df.dropna(how='all')

    # Si existe la columna 'N°', se asegura que sea numérica, pero **no filtra por ella**
    if 'N°' in df.columns:
        df['N°'] = pd.to_numeric(df['N°'], errors='coerce')
    else:
        print("⚠️ Atención: no se encontró la columna 'N°' tras limpieza.")

    return df

data = clean_multilevel_columns(dataset)
data = data.rename(columns={'N°':'ID', 'INTERNO':'INTERNO_SENAL','TIPO_ SEÑAL':'TIPO_SENAL', 'CLASE _SEÑAL':'CLASE_SENAL' })
data.columns = data.columns.str.strip().str.replace(" ", "_")
data.columns = data.columns.str.strip().str.replace(".", "_")
data.columns = data.columns.str.strip().str.replace("/", "_")
data.columns = data.columns.str.strip().str.replace("°", "_")
data.columns = data.columns.str.strip().str.replace("\n", "_")
data.columns = data.columns.str.strip().str.replace("Ñ", "N")
data.columns = data.columns.str.strip().str.replace("__", "_")

def clean_multilevel_columns_item(file_path):
    df = pd.read_excel(file_path,
                       #sheet_name="VERTICAL",
                       header=[0, 1],
                       )
    flat = df.columns.to_flat_index()
    cleaned_columns = [
        '_'.join([str(v).strip() for v in col if v and not str(v).startswith('Unnamed')])
        for col in flat
    ]

    df.columns = cleaned_columns

    # Eliminar columnas duplicadas, conservando la primera
    df = df.loc[:, ~df.columns.duplicated(keep='first')]

    # Filtrar filas si la columna 'N°' existe
    if 'N°' in df.columns:
        df = df[pd.to_numeric(df['N°'], errors='coerce').notna()]
    else:
        print("⚠️ Atención: no se encontró la columna 'N°' tras limpieza.")

    return df

#Aviriguar hoja exacta para extrae la informacion
import glob

# Identificar automáticamente el archivo de items (Items_CTO_*.xlsx).
# Si no se encuentra, buscar cualquier archivo xlsx que contenga 'item' en el nombre.
candidates = glob.glob('Items*.xlsx') + glob.glob('*Items*.xlsx')
if not candidates:
    # Buscar archivos que contengan 'item' en el nombre (case-insensitive)
    all_xlsx = glob.glob('*.xlsx')
    for f in all_xlsx:
        if 'item' in f.lower():
            candidates.append(f)
            break

if not candidates:
    print("❌ No se encontró archivo 'Items_CTO_*.xlsx' ni ningún archivo con 'item' en el nombre en el directorio. Archivos disponibles:", glob.glob('*.xlsx'))
    raise FileNotFoundError("Items_CTO file not found. Por favor coloca el archivo Items_CTO_YYYY_NNNN.xlsx en el mismo directorio.")

file_item = candidates[0]
print(f"Usando archivo de items detectado: {file_item}")

item_data = clean_multilevel_columns_item(file_item)

item_data = item_data.dropna(how='all', axis=1)

item_data = item_data.rename(columns={'ITEM':'ITEM_INTERNO'})

item_data



#Los nombres de la tabla son: "INTERNO_ITEM","UNIDAD","DESCRIPCION_ITEM","CANTIDAD","COSTOS","ANO_ITEM"

table_9 = data.copy()

table_9.columns

data['ITEM_INSTALACION_INTERNO'].value_counts()

table_9= table_9.rename(columns= {'ITEM_SUMINISTRO_INTERNO':'ITEM_INTERNO','ITEM_SUMINISTRO_CANTIDAD':'CANTIDAD'})

DF_ITEM_ANTIGRAFITI_INTERNO = data.rename(columns={'ITEM_ANTIGRAFITI_INTERNO':'ITEM_INTERNO','ITEM_ANTIGRAFITI_CANTIDAD':'CANTIDAD'})

DF_ITEM_INSTALACION_INTERNO = data.rename(columns={'ITEM_INSTALACION_INTERNO':'ITEM_INTERNO','ITEM_INSTALACION_CANTIDAD':'CANTIDAD'})

DF_ITEM_RET_REU_INTERNO = data.rename(columns={'ITEM_RET_REU_INTERNO':'ITEM_INTERNO','ITEM_RET_REU_CANTIDAD':'CANTIDAD'})

import pandas as pd

try:
    cols_to_keep = ['ITEM_INTERNO', 'CANTIDAD','INTERNO_SENAL']
    dataframes = []

    def filtrar_filas_validas(df):
        if df.empty:
            return None
        df_filtrado = df[cols_to_keep]
        # Mantener filas donde al menos una columna no sea NaN
        return df_filtrado.dropna(how='all', subset=cols_to_keep)

    for source_df in [table_9, DF_ITEM_ANTIGRAFITI_INTERNO, DF_ITEM_INSTALACION_INTERNO, DF_ITEM_RET_REU_INTERNO]:
        df_limpio = filtrar_filas_validas(source_df)
        if df_limpio is not None and not df_limpio.empty:
            dataframes.append(df_limpio)

    if dataframes:
        df_concatenado_item_int = pd.concat(dataframes, ignore_index=True)
    else:
        df_concatenado_item_int = pd.DataFrame(columns=cols_to_keep)

    print(df_concatenado_item_int)

except Exception as e:
    print(f"Error al procesar los DataFrames: {e}")
    df_concatenado_item_int = pd.DataFrame(columns=['ITEM_INTERNO', 'CANTIDAD'])

valores_unicos = df_concatenado_item_int['ITEM_INTERNO'].dropna().unique()

# 2. Crear DataFrame con esos valores
df_concatenado_item = pd.DataFrame({
    'ITEM_INTERNO': valores_unicos
})

df_concatenado_item

df_concatenado_item['ITEM_INTERNO']= df_concatenado_item['ITEM_INTERNO'].astype(int)

df_sen_item = df_concatenado_item.merge(item_data, on='ITEM_INTERNO', how='left')
df_sen_sen_item_cost = df_sen_item.copy()
df_sen_item

df_sen_item['UNIDAD'] = df_sen_item['UNIDAD'].replace({
    'ML': 'METRO LINEAL',
    'M2': 'METRO CUADRADO',
    'UND': 'UNIDAD',
    'UN': 'UNIDAD'
})
df_sen_item

#del  df_sen_item['CANTIDAD']
df_sen_item = df_sen_item.rename(columns={'ITEM_INTERNO':'INTERNO_ITEM','DESCRIPCIÓN':'DESCRIPCION_ITEM','2025_PRECIO UNITARIO INCLUYE AIU': 'COSTOS','2025_CANTIDAD':'CANTIDAD'})
df_sen_item['ANO_ITEM'] = 2025  #año de fecha elaboracion del plano

df_sen_item = df_sen_item[["INTERNO_ITEM","UNIDAD","DESCRIPCION_ITEM","CANTIDAD","COSTOS","ANO_ITEM"]]

df_sen_item['INTERNO_ITEM']

df_sen_item = df_sen_item[df_sen_item['INTERNO_ITEM'].isin(list(range(1, 29)) + [100])]

df_sen_item.drop_duplicates(subset=['INTERNO_ITEM'], inplace=True)

df_sen_item.to_excel(f'SEN_ITEM_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""# SEN_SEÑALIZACION_ITEM

"""



df_sen_item

try:
  df_concatenado_item_int['AÑO_ITEM'] = df_sen_item['ANO_ITEM'].iloc[0]
  del df_concatenado_item_int['CANTIDAD']
except:
  print('review')

df_concatenado_item_int = df_concatenado_item_int.dropna(subset=['ITEM_INTERNO'])
df_concatenado_item_int['INTERNO_SENAL'] = df_concatenado_item_int['INTERNO_SENAL'].fillna(method='ffill')
df_concatenado_item_int

try:
  df_sen_sen_item= df_concatenado_item_int.merge(table_6, on='INTERNO_SENAL', how='left')
  df_sen_sen_item
except:
  print('review')



try:
  df_sen_sen_item = df_sen_sen_item.rename(columns={'ITEM_INTERNO':'INTERNO_ITEM'})
  df_sen_sen_item= df_sen_sen_item[["INTERNO_DETALLE","INTERNO_SENAL","INTERNO_ITEM","AÑO_ITEM"]]
  #df_sen_sen_item = df_sen_sen_item.dropna()
except:
  print('review')

#df_sen_sen_item.to_excel(f'SEN_SEÑALIZACION_ITEM_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""# SEN_CONTRATO_ITEM"""

try:
  df_sen_sen_item_cost['ANO_ITEM'] = df_sen_item['ANO_ITEM'].iloc[0]
  df_sen_sen_item_cost = df_sen_sen_item_cost.rename(columns={'ITEM_INTERNO':'INTERNO_ITEM', '2025_SUBTOTAL':'COSTO_DIRECTO'})
  #"NUMERO_CONTRATO","INTERNO_ITEM","ANO_ITEM","COSTO_DIRECTO"
  df_sen_sen_item_cost["NUMERO_CONTRATO"]= df_raw_cols["NUMERO_CONTRATO"].iloc[0]
  df_sen_sen_item_cost = df_sen_sen_item_cost[["NUMERO_CONTRATO","INTERNO_ITEM","ANO_ITEM","COSTO_DIRECTO"]]


except:
  print('review')

#df_sen_sen_item_cost.to_excel(f'SEN_CONTRATO_ITEM_{df_raw_cols["NUMERO_CONTRATO"].iloc[0]}.xlsx',index=False)

"""------------------------------------------------------------------------------------------------

# Export excel file SEN
"""

try:
  output_filename = f'Combined_Output_{table_2["INTERNO_DISENO"].iloc[0]}.xlsx'

  with pd.ExcelWriter(output_filename) as writer:
      #df_raw_cols.to_excel(writer, sheet_name='CON_CONTRATO', index=False)
      #table_2.to_excel(writer, sheet_name='PRO_PROYECTO', index=False)
      #df_raw_local.to_excel(writer, sheet_name='PRO_PROYECTO_LOCALIDAD', index=False)
      #table_3.to_excel(writer, sheet_name='PRO_DETALLE_PROYECTO', index=False)
      data_geo.to_excel(writer, sheet_name='GEO_SEGMENTO', index=False)
      data_sen.to_excel(writer, sheet_name='SEN_SENALIZACION', index=False)
      table_7.to_excel(writer, sheet_name='SEN_DESCRIPCION_TABLERO', index=False)
      table_8.to_excel(writer, sheet_name='SEN_CONTENIDO_TABLERO', index=False)
      df_sen_project.to_excel(writer, sheet_name='SEN_SEÑALIZACION_PROYECTO', index=False)
      table_6.to_excel(writer, sheet_name='SEN_DETALLE_SENALIZACION', index=False)
      df_sen_item.to_excel(writer, sheet_name='SEN_ITEM', index=False)
      df_sen_sen_item.to_excel(writer, sheet_name='SEN_SEÑALIZACION_ITEM', index=False)
      df_sen_sen_item_cost.to_excel(writer, sheet_name='SEN_CONTRATO_ITEM', index=False)
      df_duplicate_tramos_sen.to_excel(writer, sheet_name='VALIDACION_DIRECCION_SEN', index=False)

  print(f"✅ All dataframes exported to {output_filename}")
except:
  print('No se genero archivo')

"""## DEMARCACIÓN"""

#Read excel file headers
df = pd.read_excel(dataset,
                   sheet_name='DEMARCACION',
                   header=None,
                   nrows=5
                   #index_col=0
                   )
del df[2]
df = df.iloc[1:,1:4].T.dropna()
df_raw = (df
          .copy()
          .rename(columns=df.iloc[0]).drop(index=df.index[0])
          .reset_index(drop=True)
)
df_raw

import pandas as pd
import re


df_raw = df_raw.rename(columns={'NOMBRE PLANO':'INTERNO_DISENO', 'CONTRATO':'NUMERO_CONTRATO'})
df_raw
def formatear_numero_contrato(df):
    def transformar(valor):
        # Extrae los números usando regex
        numeros = re.findall(r'\d+', str(valor))
        if len(numeros) == 2:
            return f'CTO {numeros[0]} DE {numeros[1]}'
        return valor  # Si no coincide, devuelve el valor original

    df['NUMERO_CONTRATO'] = df['NUMERO_CONTRATO'].apply(transformar)
    return df
df_raw = formatear_numero_contrato(df_raw)

df_raw

test = df_raw.merge(
    df_interno,
    on='INTERNO_DISENO',
    how='left'
)
test

import pandas as pd

def clean_multilevel_columns(file_path):
    df = pd.read_excel(
        file_path,
        sheet_name="DEMARCACION",
        header=[7, 8],  # Lee multi-nivel
    )

    # Aplana el MultiIndex en nombres únicos
    flat = df.columns.to_flat_index()
    cleaned = [
        '_'.join([str(v).strip() for v in tup if v and not str(v).startswith('Unnamed')])
        for tup in flat
    ]

    df.columns = cleaned

    # ---- Manejo especial de duplicados ----
    # Si hay varias columnas llamadas 'OBSERVACIONES', se unifican en una sola
    if df.columns.duplicated().any():
        duplicates = df.columns[df.columns.duplicated()].unique()
        for col in duplicates:
            # Combina en una sola (mantiene el primero, rellena con los otros)
            df[col] = df.filter(like=col).bfill(axis=1).iloc[:, 0]
            # Elimina las duplicadas
            df = df.loc[:, ~df.columns.duplicated()]

    # ---- Filtro de filas válidas según 'N°' ----
    if 'N°' in df.columns:
        df = df[pd.to_numeric(df['N°'], errors='coerce').notna()]
    else:
        print("⚠️ Atención: no se encontró la columna 'N°' tras limpieza.")

    return df

"""

def clean_multilevel_columns(file_path):
    df = pd.read_excel(file_path,
                       sheet_name="DEMARCACION",
                       header=[7,8],
                       )

    # Aplana el MultiIndex en tuplas
    flat = df.columns.to_flat_index()
    # Une los niveles omitiendo partes vacías o "Unnamed"
    cleaned = [
        '_'.join([str(v).strip() for v in tup if v and not str(v).startswith('Unnamed')])
        for tup in flat
    ]
    df.columns = cleaned
    if 'N°' in df.columns:
        df = df[pd.to_numeric(df['N°'], errors='coerce').notna()]
    else:
        print("⚠️ Atención: no se encontró la columna 'N°' tras limpieza.")

    return df

"""

try:
  data = clean_multilevel_columns(dataset)
  if data.empty:
    print("❗ DataFrame 'data' está vacío. Por favor, cargue un nuevo dataset en la celda 2 para continuar.")
    raise SystemExit("DataFrame vacío, deteniendo ejecución.") # Stop execution if DataFrame is empty
  else:
    print("✅ DataFrame 'data' cargado correctamente:")
    #display(data.head()) # Use display for better formatting
except Exception as e:
  print(f"⚠️ Error al cargar o procesar el archivo: {e}")
  print("Por favor, verifica el nombre del archivo o la hoja y vuelve a intentarlo.")
  raise SystemExit("Error al cargar datos, deteniendo ejecución.") # Stop execution on error

data.isna().sum()

# Create a boolean mask for rows where the specified columns have duplicates within the row
mask_duplicates = data.apply(lambda row: len(set([row['TRAMO_EJE.1'], row['TRAMO_INICIO.1'], row['TRAMO_TERMINA.1']])) < 3, axis=1)

# Filter the DataFrame using the mask and display all columns
df_duplicate_tramos = data.loc[mask_duplicates, :].copy()

#Verificar dirección
df_duplicate_tramos

#adress
def build_addres(row):
    parte_eje = f"{row['TRAMO_EJE']} {row['TRAMO_EJE.1']}"
    parte_inicio = f"{row['TRAMO_INICIO']}{row['TRAMO_INICIO.1']}"
    parte_termina = f"{row['TRAMO_TERMINA']}{row['TRAMO_TERMINA.1']}"
    # Aquí defines tu frase tipo: #eje {parte_eje}
    return f"{parte_eje} DESDE {parte_inicio}  HASTA {parte_termina}"

data['DIRECCION'] = data.apply(build_addres, axis=1)

# results:
#data[['DIRECCION']].head()

data = data.rename(columns={'N°':'ID','INTERNO':'INTERNO_DEMARCACION','CLASE_MARCA':'CLASE_DEMARCACION'})

df = data.copy()
df['ID'] = df['ID'].astype(int)
df['INTERNO_DEMARCACION'] = df['INTERNO_DEMARCACION'].astype(int)
#df['CLASE_DEMARCACION'] = df['CLASE_DEMARCACION'].astype(str)
df['DIRECCION'] = df['DIRECCION'].astype(str)
#df = data.rename(columns={'CLASE_DEMARCACION':'CLASE_DEMARCACION_1'})
df.columns

try:
    cursor = conn.cursor()

    # Nombre de la tabla y nueva columna
    tabla = "DEM_DEMARCACION"
    nueva_col = "CLASE_DEMARCACION"
    tipo = "TEXT(50)"  # Puedes cambiar a NUMBER, DATETIME, según necesites

    # Intenta agregar la columna
    try:
        #sql = f"ALTER TABLE {tabla} ADD COLUMN {nueva_col} {tipo}"
        sql = f"ALTER TABLE {tabla} DROP COLUMN {nueva_col} {tipo}"
        cursor.execute(sql)
        conn.commit()
        print(f"✅ Columna '{nueva_col}' añadida correctamente como {tipo}")
    except pyodbc.ProgrammingError as e:
        print("❌ Error al añadir la columna:", e)
except Exception as e:
    print("An error occurred:", e)

df.columns

try:
  # Lista de columnas para insertar ,'CLASE_DEMARCACION'
  columnas = ['ID','INTERNO_DEMARCACION','CLASE_DEMARCACION','DIRECCION']

  # SQL de inserción
  insert_sql = f"""
  INSERT INTO DEM_DEMARCACION(
      {', '.join(f'[{col}]' for col in columnas)}
  ) VALUES ({', '.join(['?'] * len(columnas))})
  """

  # Insertar datos desde el DataFrame
  for _, row in df.iterrows():
      values = [row.get(col, None) for col in columnas]
      cursor.execute(insert_sql, values)

  # Finalizar conexión
  conn.commit()

  print("✅ Datos insertados exitosamente.")
except:
  print('No se han insertado datos')

# Find duplicate 'INTERNO_DEMARCACION' values
duplicate_internos = df[df.duplicated(subset=['INTERNO_DEMARCACION'], keep=False)]['INTERNO_DEMARCACION'].unique()

# Create a mask to filter out rows that have duplicate 'INTERNO_DEMARCACION' and 'ACCION' as 'BORRAR'
mask_to_remove = (df['INTERNO_DEMARCACION'].isin(duplicate_internos)) & (df['ACCION'] == 'BORRAR')

# Apply the mask to keep rows that are NOT in the mask_to_remove
df_dem_demarcacion = df[~mask_to_remove].copy()

df_dem_demarcacion = df[columnas]

df_dem_demarcacion

"""## DEM_DEMARCACION_DETALLE"""

try:
    cursor = conn.cursor()

    # Nombre de la tabla y nueva columna
    tabla = "DEM_DEMARCACION_DETALLE"
    nueva_col = "CLASE_DEMARCACION_1"
    tipo = "TEXT(50)"  # Puedes cambiar a NUMBER, DATETIME, según necesites

    # Intenta agregar la columna
    try:
        sql = f"ALTER TABLE {tabla} ADD COLUMN {nueva_col} {tipo}"
        #sql = f"ALTER TABLE {tabla} DROP COLUMN {nueva_col} {tipo}"
        cursor.execute(sql)
        conn.commit()
        print(f"✅ Columna '{nueva_col}' añadida correctamente como {tipo}")
    except pyodbc.ProgrammingError as e:
        print("❌ Error al añadir la columna:", e)
except Exception as e:
    print("An error occurred:", e)

try:
    cursor = conn.cursor()

    # Nombre de la tabla que quieres inspeccionar
    table_name = "DEM_DEMARCACION_DETALLE"

    # Itera sobre columnas usando el método estándar .columns()
    print(f"Esquema de la tabla '{table_name}':")
    for col in cursor.columns(table=table_name):
        nombre = col.column_name
        tipo = col.type_name  # nombre legible del tipo en Access
        tamaño = col.column_size
        nullable = col.nullable  # 1 = acepta NULL, 0 = NOT NULL
        print(f" - {nombre}: {tipo}({tamaño}) nullable={nullable}")

except Exception as e:
    print("An error occurred:", e)

data = data.rename(columns={'UNIDAD\nCANTIDAD':'CANTIDAD', 'GARANTIA\n(MESES)':'GARANTIA',
                             'FECHA_VENCIMIENTO\n_GARANTIA':'FECHA_VENCIMIENTO_GARANTIA',
                             'NUMERO\n_CUENTA':'NUMERO_CUENTA',
                             'ESTADO':'ESTADO_DEMARCACION', 'TIPO_ MEDIDA':'TIPO_MEDIDA',
                             'PINTURA_COLOR':'COLOR_MATERIAL'
                             })

data.columns

# 1. Convertir al tipo correcto:
df['ID'] = df['ID'].astype(float)
df['INTERNO_DEMARCACION'] = df['INTERNO_DEMARCACION'].astype(float)
df['CLASE_DEMARCACION'] = df['CLASE_DEMARCACION'].astype(str)
#df['FASE'] = df['FASE'].astype(float)
#df['ACCION'] = df['ACCION'].astype(float)
#df['ESTADO_DEMARCACION'] = df['ESTADO_DEMARCACION'].astype(float)
df['FECHA_FASE'] = pd.to_datetime(df['FECHA_FASE'])
#df['NUMERO_CUENTA'] = df['NUMERO_CUENTA'].astype('Int64')  # permite nulos
#df['TIPO_MEDIDA'] = df['TIPO_MEDIDA'].astype(float)
#df['CANTIDAD'] = df['CANTIDAD'].astype(float)
#df['COLOR_MATERIAL'] = df['COLOR_MATERIAL'].astype(float)
#df['GARANTIA'] = df['GARANTIA'].astype(float)
#df['FECHA_VENCIMIENTO_GARANTIA'] = pd.to_datetime(df['FECHA_VENCIMIENTO_GARANTIA'])
#df['OBSERVACIONES'] = df['OBSERVACIONES'].astype(str)

try:
    # Columnas de df que quieres leer
    df_cols = ['ID', 'INTERNO_DEMARCACION', 'CLASE_DEMARCACION_1', 'CIV', 'TRAMO_EJE',
           'TRAMO_EJE.1', 'TRAMO_INICIO', 'TRAMO_INICIO.1', 'TRAMO_TERMINA',
           'TRAMO_TERMINA.1', 'FASE', 'ACCION', 'ESTADO_DEMARCACION', 'FECHA_FASE',
           'NUMERO_CUENTA', 'TIPO_MEDIDA', 'CANTIDAD', 'PINTURA_CANTIDAD',
           'COLOR_MATERIAL', 'IMPRIMANTE_CANTIDAD', 'IMPRIMANTE_COLOR',
           'ANTIDESLIZANTE_INTERNO', 'ANTIDESLIZANTE_CANTIDAD', 'GARANTIA',
           'FECHA_VENCIMIENTO_GARANTIA', 'INTERNO_ITEM_PINTURA',
           'INTERNO_ITEM_IMPRIMANTE', 'INTERNO_ITEM_INSTALACION', 'OBSERVACIONES',
           'DIRECCION']

    # 1) Leer metadata para obtener columnas disponibles
    available = [col.column_name for col in cursor.columns(table='DEM_DEMARCACION_DETALLE')]

    # 2) Encontrar intersección
    sel = [c for c in df_cols if c in available]
    if not sel:
        raise ValueError("No se encontraron columnas coincidentes en la tabla Access.")

    # 3) Construir y ejecutar SELECT
    cols_sql = ", ".join(f"[{c}]" for c in sel)
    sql = f"SELECT {cols_sql} FROM DEM_DEMARCACION_DETALLE"
    cursor.execute(sql)
    rows = cursor.fetchall()

    # 4) Obtener nombres con description
    names = [col[0] for col in cursor.description]
    df = pd.DataFrame.from_records(rows, columns=names)

    print(f"Columnas leídas ({len(df.columns)}):", df.columns.tolist())
    print("Primeras filas:\n", df.head())

    cursor.close()
    conn.close()

except Exception as e:
    print("An error occurred:", e)

#df['CLASE_DEMARCACION_1'] = df['CLASE_DEMARCACION_1'].astype(str)

#color de celda vacia cambia a no aplica
#df['']
#si el campo cantidad pintura esta vacio se coloca el valor unitario

#df['CLASE_DEMARCACION_1'] = df['CLASE_DEMARCACION']

df = data.copy()

##### corregir aquí
df['NUMERO_CONTRATO'] =  df_raw['NUMERO_CONTRATO'].iloc[0] #'CTO 2021 DE 2025'
df['FECHA_REGISTRO'] = date.today()

df[['ID', 'INTERNO_DEMARCACION', 'CLASE_DEMARCACION','NUMERO_CONTRATO', 'FASE', 'ACCION',
           'ESTADO_DEMARCACION', 'FECHA_FASE', 'NUMERO_CUENTA', 'TIPO_MEDIDA', 'CANTIDAD',
           'COLOR_MATERIAL', 'GARANTIA', 'FECHA_VENCIMIENTO_GARANTIA', 'OBSERVACIONES','FECHA_REGISTRO','CIV']]



import pandas as pd

# 1. Crear máscara para BORRAR con INTERNO_DEMARCACION vacío
mask = df['INTERNO_DEMARCACION'].isna() & (df['ACCION'] == 'BORRAR')

# 2. Rellenar esos valores con el anterior válido en la misma columna
df.loc[mask, 'INTERNO_DEMARCACION'] = (
    df['INTERNO_DEMARCACION'].fillna(method='ffill')
)

# 3. Identificar duplicados basados en columnas clave (por ejemplo: 'INTERNO_DEMARCACION' y lo que defina los grupos)
cols_clave = ['INTERNO_DEMARCACION']#, 'otra_columna_relacionada']  # Ajusta según tus datos

# 4. Para cada grupo duplicado, ordenar para que BORRAR aparezca primero
df = (
    df
    .sort_values(by=cols_clave + ['ACCION'], key=lambda col: col == 'BORRAR', ascending=False)
    .reset_index(drop=True)
)

df.columns

df[df['TIPO_MEDIDA']== 'UNIDAD'].isna().sum()

df['INTERNO_ITEM_INSTALACION'].value_counts()

df['INTERNO_ITEM_PINTURA'].value_counts()

df['INTERNO_ITEM_IMPRIMANTE'].value_counts()

df['PINTURA_CANTIDAD'] = df['PINTURA_CANTIDAD'].fillna(df['CANTIDAD'])

#df['CANTIDAD'] = df['CANTIDAD'].fillna(df['PINTURA_CANTIDAD'])

df.isna().sum()

mask = df['ANTIDESLIZANTE_INTERNO'].notna() & (df['ANTIDESLIZANTE_INTERNO'].astype(str).str.strip() != '')
df_filtrado_anti = df[mask].copy()

df_filtrado_anti['COLOR_MATERIAL']= 'TRANSPARENTE'

df_filtrado_anti['OBSERVACIONES'] ='ANTIDESLIZANTE'

df_filtrado_anti['CANTIDAD'] = df_filtrado_anti['ANTIDESLIZANTE_CANTIDAD']

df_filtrado_anti.info()

df.columns

mask = df['IMPRIMANTE_COLOR'].notna() & (df['IMPRIMANTE_COLOR'].astype(str).str.strip() != '')
df_filtrado = df[mask].copy()

df_filtrado.columns

df_filtrado['IMPRIMANTE_CANTIDAD'].isna().sum()

df_filtrado['COLOR_MATERIAL']= 'NEGRO'

df_filtrado.isna().sum()

df_filtrado['CLASE_DEMARCACION'] = 'I' + df['CLASE_DEMARCACION']

df_filter = df_filtrado[['ID', 'INTERNO_DEMARCACION', 'CLASE_DEMARCACION','NUMERO_CONTRATO', 'FASE', 'ACCION',
           'ESTADO_DEMARCACION', 'FECHA_FASE', 'NUMERO_CUENTA', 'TIPO_MEDIDA', 'IMPRIMANTE_CANTIDAD',
           'COLOR_MATERIAL', 'GARANTIA', 'FECHA_VENCIMIENTO_GARANTIA', 'OBSERVACIONES','FECHA_REGISTRO','CIV']]

df_filter['IMPRIMANTE_CANTIDAD']

df_filter= df_filter.rename(columns={'IMPRIMANTE_CANTIDAD':'CANTIDAD'})

df_filter.isna().sum()

df_filter

#df_filter_paint.isna().sum()

df_filter_paint = df[['ID', 'INTERNO_DEMARCACION', 'CLASE_DEMARCACION','NUMERO_CONTRATO', 'FASE', 'ACCION',
           'ESTADO_DEMARCACION', 'FECHA_FASE', 'NUMERO_CUENTA', 'TIPO_MEDIDA', 'PINTURA_CANTIDAD',
           'COLOR_MATERIAL', 'GARANTIA', 'FECHA_VENCIMIENTO_GARANTIA', 'OBSERVACIONES','FECHA_REGISTRO','CIV']]

df_filter_paint= df_filter_paint.rename(columns={'PINTURA_CANTIDAD':'CANTIDAD'})

"""# Asegurar que no hay nombres de columnas duplicados
df = df.loc[:, ~df.columns.duplicated()].copy()
df_filter = df_filter.loc[:, ~df_filter.columns.duplicated()].copy()

# Asegurar mismo orden de columnas
df_filter = df_filter[df.columns.intersection(df_filter.columns)]

# Resetear índice y concatenar
df_total = pd.concat(
    [df.reset_index(drop=True), df_filter.reset_index(drop=True)],
    ignore_index=True
)
"""

#df_total = pd.concat([df, df_filter], ignore_index=True)

df_filter_paint = df_filter_paint.dropna(subset=['CANTIDAD'])
df_filter = df_filter.dropna(subset=['CANTIDAD'])
#df_antides_f = df_antides_f.dropna(subset=['CANTIDAD'])

df_total = pd.concat([df_filter.reset_index(drop=True),
                      df_filter_paint.reset_index(drop=True)
                      ],
                     ignore_index=True)

df_total

df_filtrado_anti

try:
  df_total = pd.concat([df_total,df_filtrado_anti],ignore_index=True)
except:
  print('no se concatena')

#del df_filtrado_anti['CLASE_DEMARCACION']

df_filtrado_anti = df_filtrado_anti.rename(columns={'CLASE_DEMARCACION_1':'CLASE_DEMARCACION'})

df_filtrado_anti =df_filtrado_anti[[ 'ID', 'INTERNO_DEMARCACION', 'CLASE_DEMARCACION','NUMERO_CONTRATO', 'FASE', 'ACCION',
           'ESTADO_DEMARCACION', 'FECHA_FASE', 'NUMERO_CUENTA', 'TIPO_MEDIDA', 'CANTIDAD',
           'COLOR_MATERIAL', 'GARANTIA', 'FECHA_VENCIMIENTO_GARANTIA', 'OBSERVACIONES','FECHA_REGISTRO']]

df_filtrado_anti.columns

df_total['INTERNO_DETALLE'] = range(1, len(df_total) + 1)

df_total.isna().sum()

df_unido_fin = df_total[[
    'INTERNO_DETALLE','ID', 'INTERNO_DEMARCACION', 'CLASE_DEMARCACION',
    'NUMERO_CONTRATO', 'FASE', 'ACCION', 'FECHA_FASE','ESTADO_DEMARCACION',
    'TIPO_MEDIDA', 'COLOR_MATERIAL', 'CANTIDAD', 'GARANTIA',
    'FECHA_VENCIMIENTO_GARANTIA', 'NUMERO_CUENTA', 'FECHA_REGISTRO',
    'OBSERVACIONES','CIV'
]]

df_unido_fin

df_unido_fin.info()

df_unido_fin['COLOR_MATERIAL'] = df_unido_fin['COLOR_MATERIAL'].fillna('NO APLICA')

df_unido_fin

df_unido_fin['FECHA_FASE']= pd.to_datetime(df_unido_fin['FECHA_FASE'],errors='coerce')

df_unido_fin['FECHA_FASE'] = df_unido_fin['FECHA_FASE'].dt.strftime('%d/%m/%Y')

df_unido_fin['FECHA_REGISTRO'] = pd.to_datetime(df_unido_fin['FECHA_REGISTRO'], errors='coerce')

df_unido_fin['FECHA_REGISTRO_FORMAT'] = df_unido_fin['FECHA_REGISTRO'].dt.strftime('%d/%m/%Y')

df_unido_fin['FECHA_REGISTRO_FORMAT']
del df_unido_fin['FECHA_REGISTRO']
df_unido_fin = df_unido_fin.rename(columns={'FECHA_REGISTRO_FORMAT':'FECHA_REGISTRO'})

df_unido_fin['FECHA_REGISTRO']

df_unido_fin['FECHA_VENCIMIENTO_GARANTIA'] = pd.to_datetime(
    df_unido_fin['FECHA_VENCIMIENTO_GARANTIA']
)
df_unido_fin['FECHA_VENCIMIENTO_GARANTIA'] = df_unido_fin[
    'FECHA_VENCIMIENTO_GARANTIA'
].dt.strftime('%d/%m/%Y')

df_unido_fin['FECHA_VENCIMIENTO_GARANTIA']

#DEM_DEMARCACION_DETALLE = df_unido_fin.copy()
#del DEM_DEMARCACION_DETALLE['CIV']

#df_unido_fin.to_excel('DEM_DEMARCACION_DETALLE_1.xlsx')

#del df_unido['CLASE_DEMARCACION']

try:
    columnas =['ID', 'INTERNO_DEMARCACION', 'CLASE_DEMARCACION_1','NUMERO_CONTRATO', 'FASE', 'ACCION',
               'ESTADO_DEMARCACION', 'FECHA_FASE', 'NUMERO_CUENTA', 'TIPO_MEDIDA', 'CANTIDAD',
               'COLOR_MATERIAL', 'GARANTIA', 'FECHA_VENCIMIENTO_GARANTIA', 'OBSERVACIONES','FECHA_REGISTRO']

    insert_sql = f"""
    INSERT INTO DEM_DEMARCACION_DETALLE (
        {', '.join(f'[{col}]' for col in columnas)}
    ) VALUES ({', '.join(['?'] * len(columnas))})
    """

    # Opcional: con autocommit=False, asegúrate de llamar conn.commit() al final
    for _, row in df.iterrows():
        values = [row.get(col, None) for col in columnas]
        try:
            cursor.execute(insert_sql, values)
        except Exception as e:
            print(f"⚠️ Error inserting row: {values}. Error: {e}")


    if not conn.autocommit:
        conn.commit()  # <— No olvides los paréntesis aquí

    total_inserted = 0
    for _, row in df.iterrows():
        values = [row.get(col, None) for col in columnas]
        try:
            cursor.execute(insert_sql, values)
            # sumar filas afectadas (debería ser 1 por cada INSERT)
            count = cursor.rowcount
            if count is None or count < 0:
                # rowcount puede devolver -1 si no es determinable, pero en INSERT debería ser 1
                print("⚠️ Advertencia: rowcount no disponible tras INSERT.")
            else:
                total_inserted += count
        except Exception as e:
            print(f"⚠️ Error inserting row: {values}. Error: {e}")


    print(f"✅ Inserción completada. Filas insertadas: {total_inserted}")
except Exception as e:
    print("An error occurred:", e)

"""try:
    # Confirmación previa (opcional)
    resp = input("¿Eliminar todos los registros de DEM_DEMARCACION? (s/n): ").strip().lower()
    if resp != 's':
        print("Operación cancelada.")
    else:
        sql = "DELETE * FROM DEM_DEMARCACION"
        cursor.execute(sql)
        conn.commit()
        print(f"✅ Registro(s) eliminados: {cursor.rowcount}")
except Exception as e:
    conn.rollback()
    print("❌ Error al eliminar registros:", e)

try:
    tabla = "DEM_DEMARCACION"
    columna = "CLASE_DEMARCACION"

    # 1) Encontrar índices asociados a la columna
    indices = []
    for row in cursor.statistics(table=tabla, unique=False, quick=False):
        idx_name = row.index_name
        col_name = row.column_name
        if col_name == columna and idx_name:
            indices.append(idx_name)
    indices = list(set(indices))

    # 2) Eliminar cada índice
    for idx in indices:
        try:
            cursor.execute(f"DROP INDEX {idx} ON {tabla}")
            print(f"✔ Índice eliminado: {idx}")
        except Exception as e:
            print(f"⚠️ No se pudo eliminar índice {idx}: {e}")

    # 3) Encontrar relaciones / constraints (foreign keys)
    #    Access usa DROP CONSTRAINT en SQL via ODBC para eliminar relaciones :contentReference[oaicite:1]{index=1}
    # Necesitas conocer el nombre del constraint; asumimos que es 'FK_' + tabla + '_' + columna
    fk_name = f"FK_{tabla}_{columna}"

    # 4) Intentar eliminar la constraint
    try:
        cursor.execute(f"ALTER TABLE {tabla} DROP CONSTRAINT {fk_name}")
        print(f"✔ Relación eliminada: {fk_name}")
    except Exception as e:
        print(f"⚠️ No se pudo eliminar relación {fk_name}: {e}")

    # 5) Ahora se puede eliminar la columna
    try:
        cursor.execute(f"ALTER TABLE {tabla} DROP COLUMN {columna}")
        conn.commit()
        print(f"✅ Columna eliminada: {columna}")
    except Exception as e:
        print(f"❌ No se pudo eliminar columna {columna}: {e}")

    cursor.close()
    conn.close()
except Exception as e:
    print("An error occurred:", e)
"""

# 1. Extraer el valor único
valor_proyecto = test.loc[0, 'INTERNO_PROYECTO']

# 2. Asignarlo a toda la columna
df_unido_fin['INTERNO_PROYECTO'] = valor_proyecto

###### aqui va la restricción ...
# Create a list of the values to check against
demarcation_classes_ye = ["BL", "DS", "LC", "LC_C", "LCM", "LCM_C", "LPA_C", "PSR34", "RE", "RTC", "TCH", "ZB"
]

# Use .loc to conditionally update the 'COLOR_MATERIAL' column
df_unido_fin.loc[df_unido_fin['CLASE_DEMARCACION'].isin(demarcation_classes_ye), 'COLOR_MATERIAL'] = 'AMARILLO'

###### aqui va la restricción ...
# Create a list of the values to check against
demarcation_classes = ['PCC','PCP', 'RPA', 'PSP47A']

# Use .loc to conditionally update the 'COLOR_MATERIAL' column
df_unido_fin.loc[df_unido_fin['CLASE_DEMARCACION'].isin(demarcation_classes), 'COLOR_MATERIAL'] = 'AMARILLO-NEGRO'

df_unido_fin

df_unido_fin[df_unido_fin['TIPO_MEDIDA']=='UNIDAD']

#keep_columns =['ID', 'INTERNO_DEMARCACION', 'CLASE_DEMARCACION','NUMERO_CONTRATO', 'FASE', 'ACCION',
 #              'ESTADO_DEMARCACION', 'FECHA_FASE', 'NUMERO_CUENTA', 'TIPO_MEDIDA', 'CANTIDAD',
  #             'COLOR_MATERIAL', 'GARANTIA', 'FECHA_VENCIMIENTO_GARANTIA', 'OBSERVACIONES','FECHA_REGISTRO']

keep_columns = ["INTERNO_DETALLE","ID","INTERNO_DEMARCACION","CLASE_DEMARCACION","NUMERO_CONTRATO","FASE","ACCION",
                "FECHA_FASE","ESTADO_DEMARCACION","TIPO_MEDIDA","COLOR_MATERIAL","CANTIDAD","GARANTIA","FECHA_VENCIMIENTO_GARANTIA",
                "NUMERO_CUENTA","FECHA_REGISTRO","OBSERVACIONES"]

df_dem_detalle_to_export = df_unido_fin.copy()
df_dem_detalle_to_export = df_dem_detalle_to_export[keep_columns]
#df_dem_detalle_to_export.to_excel('DEM_DEMARCACION_DETALLE_1.xlsx')

mask = (
    (df_dem_detalle_to_export['ACCION'] == 'BORRAR') &
    (df_dem_detalle_to_export['ESTADO_DEMARCACION'] == 'MALO')
)

# --- Paso 3: Aplicar la asignación solo si existen filas que cumplen la condición ---
if mask.any():
    df_dem_detalle_to_export.loc[mask, 'GARANTIA'] = 0

DEM_DEMARCACION_DETALLE =df_dem_detalle_to_export.copy()
# =df_dem_detalle_to_export[(df_dem_detalle_to_export['ACCION']=='BORRAR' & df_dem_detalle_to_export['ESTADO_DEMARCACION'] == 'MALO')]

DEM_DEMARCACION_DETALLE

"""# DEM DEMARCACION SEGMENTO"""

try:
  civ_dem = pd.read_excel(dataset,sheet_name='CIV_DEM')

except:
  print('no hay datos')

civ_dem.dropna(inplace=True)

civ_dem = civ_dem.rename(columns={'CONSECUTIVO':'ID', 'INTERNO_DEMARCACIÓN ': 'INTERNO_DEMARCACION' })

civ_dem.columns

civ_dem

df_dem_demarcacion_segmento = df_unido_fin[['ID','INTERNO_DEMARCACION','CIV']]

df_dem_demarcacion_segmento = df_dem_demarcacion_segmento.drop_duplicates()

df_dem_demarcacion_segmento.info()

try:
  df_dem_demarcacion_segmento = pd.concat([df_dem_demarcacion_segmento, civ_dem], ignore_index=True)
  if civ_dem.empty:
   print (df_dem_demarcacion_segmento)
except:
  print('review')

df_dem_demarcacion_segmento.isna().sum()

df_dem_demarcacion_segmento.info()

"""# DEM DEMARCACION PROYECTO

"""

df_dem_demarcacion_proyecto = df_unido_fin[['INTERNO_DETALLE','INTERNO_DEMARCACION','CLASE_DEMARCACION']]#,'INTERNO_PROYECTO']]
df_dem_demarcacion_proyecto

# 1. Extraer el valor único
valor_proyecto = test.loc[0, 'INTERNO_PROYECTO']

# 2. Asignarlo a toda la columna
df_dem_demarcacion_proyecto['INTERNO_PROYECTO'] = valor_proyecto

df_dem_demarcacion_proyecto

df.columns

"""# DEM_ITEM"""

df.columns

#Recordar esta restricción porque se puso fillna ? con cantidad pintura ? error?
#df['INTERNO_ITEM_PINTURA'] = df['INTERNO_ITEM_PINTURA'].fillna(df['PINTURA_CANTIDAD'])

#df['INTERNO_ITEM_PINTURA'] = df['INTERNO_ITEM_PINTURA'].fillna(df['PINTURA_CANTIDAD'])



try:
    # Seleccionar columnas si existen
    item_pintura = df[['INTERNO_ITEM_PINTURA', 'INTERNO_DEMARCACION']]

    # Si toda la columna INTERNO_ITEM_PINTURA es NaN
    if item_pintura['INTERNO_ITEM_PINTURA'].isna().all():
        print("La columna INTERNO_ITEM_PINTURA está vacía")
    else:
        # Eliminar filas con NaN en esas columnas
        item_pintura = item_pintura.dropna()
        print("Filtrado sin NaN:")
        print(item_pintura)

except KeyError:
    # Si alguna de las columnas no existe
    print("No existen las columnas en el DataFrame")

try:
    # Mask para INTERNO_ITEM_IMPRIMANTE
    mask = df['INTERNO_ITEM_IMPRIMANTE'].notna() & (df['INTERNO_ITEM_IMPRIMANTE'].astype(str).str.strip() != '')

    if mask.any():
        # Si hay valores válidos en INTERNO_ITEM_IMPRIMANTE
        df_imprimante = df[mask].copy()
    else:
        # Si está vacío, validar INTERNO_ITEM_INSTALACION
        mask_insta = df['INTERNO_ITEM_INSTALACION'].notna() & (df['INTERNO_ITEM_INSTALACION'].astype(str).str.strip() != '')
        if mask_insta.any():
            df_imprimante = df[mask_insta].copy()
            # Llenar INTERNO_ITEM_IMPRIMANTE con los valores de INTERNO_ITEM_INSTALACION
            df_imprimante['INTERNO_ITEM_IMPRIMANTE'] = df_imprimante['INTERNO_ITEM_INSTALACION']
        else:
            df_imprimante = pd.DataFrame()  # Si ambos están vacíos devolvemos df vacío
except Exception as e:
    print("Error en validación:", e)
    df_imprimante = pd.DataFrame()

"""try:
  mask = df['INTERNO_ITEM_IMPRIMANTE'].notna() & (df['INTERNO_ITEM_IMPRIMANTE'].astype(str).str.strip() != '')
  if mask.empty:
    mask_insta = df['INTERNO_ITEM_IMPRIMANTE'].notna() &(df['INTERNO_ITEM_INSTALACION'])
    df_imprimante = df[mask_insta].copy()
  else:
    print(mask_insta)
except:
  df_imprimante #= df[mask].copy()
"""

df_imprimante = df_imprimante[['INTERNO_ITEM_IMPRIMANTE','INTERNO_DEMARCACION']]

df_imprimante.isna().sum()

df_imprimante

mask = df['ANTIDESLIZANTE_INTERNO'].notna() & (df['ANTIDESLIZANTE_INTERNO'].astype(str).str.strip() != '')
df_antides = df[mask].copy()

df_antides['ANTIDESLIZANTE_INTERNO'] = '68'

df_antides = df_antides[['ANTIDESLIZANTE_INTERNO','INTERNO_DEMARCACION']]

item_pintura = item_pintura.rename(columns={'INTERNO_ITEM_PINTURA':'ITEM'})

df_antides = df_antides.rename(columns={'ANTIDESLIZANTE_INTERNO':'ITEM'})

try:
  #rename columns
  df_imprimante = df_imprimante.rename(columns= {'INTERNO_ITEM_IMPRIMANTE':'ITEM'})
except:
  print( 'no data')

df_imprimante

df.columns

##aqui!!
mask_11 = df['INTERNO_ITEM_INSTALACION'].notna() & (df['INTERNO_ITEM_INSTALACION'].astype(str).str.strip() != '')

df_install = df[mask_11].copy()

df_install = df_install[['INTERNO_ITEM_INSTALACION','INTERNO_DEMARCACION']]
df_install = df_install.rename(columns={'INTERNO_ITEM_INSTALACION':'ITEM'})

#df_install = df_install[['INTERNO_ITEM_INSTALACION','INTERNO_DEMARCACION']]

df_install

df_concat = pd.concat(
    [
      item_pintura,
      df_imprimante,
      df_antides,
      df_install
    ],
    ignore_index=True
)

df_concat

import pandas as pd

try:
    cols_to_keep = ['ITEM','INTERNO_DEMARCACION']
    dataframes = []

    def filtrar_filas_validas(df):
        if df.empty:
            return None
        df_filtrado = df[cols_to_keep]
        # Mantener filas donde al menos una columna no sea NaN
        return df_filtrado.dropna(how='all', subset=cols_to_keep)

    for source_df in [item_pintura, df_antides, df_imprimante,df_install]:
        df_limpio = filtrar_filas_validas(source_df)
        if df_limpio is not None and not df_limpio.empty:
            dataframes.append(df_limpio)

    if dataframes:
        df_concatenado_dem_int = pd.concat(dataframes, ignore_index=True)
    else:
        df_concatenado_dem_int = pd.DataFrame(columns=cols_to_keep)

    print(df_concatenado_dem_int)

except Exception as e:
    print(f"Error al procesar los DataFrames: {e}")
    df_concatenado_dem_int = pd.DataFrame(columns=['ITEM_INTERNO', 'CANTIDAD'])

df_install['ITEM'].value_counts()

cols_to_keep = ['ITEM']#, 'INTERNO_ITEM_IMPRIMANTE', 'ANTIDESLIZANTE_INTERNO']


item_pintura_f = item_pintura[cols_to_keep].dropna(subset=['ITEM'])
df_imprimante_f = df_imprimante[cols_to_keep].dropna(subset=['ITEM'])
df_antides_f   = df_antides[cols_to_keep].dropna(subset=['ITEM'])
df_install   = df_install[cols_to_keep].dropna(subset=['ITEM'])
# --- Concatenar ---
df_concatenado = pd.concat(
    [item_pintura_f, df_imprimante_f, df_antides_f,df_install],
    ignore_index=True
)



"""df_concatenado = pd.concat(
    [item_pintura[cols_to_keep],
     df_imprimante[cols_to_keep],
     df_antides[cols_to_keep]],
    ignore_index=True
)
"""

df_concatenado.value_counts()

df_unido_fin['INTERNO_ITEM'] = df_concatenado['ITEM']

df_unido_fin['INTERNO_ITEM'].sort_values(ascending=True)
df_unido_fin['INTERNO_ITEM'].astype(int).value_counts()

df_unido_fin

df_unido_fin.columns

df_unido_fin.isna().sum()

# Filtrar filas donde INTERNO_ITEM es NaN o ±inf
#df_invalidos = df_unido_fin[df_unido_fin['INTERNO_ITEM'].isna() |
 #                           np.isinf(df_unido_fin['INTERNO_ITEM'])]

# Mostrar resultado
#df_invalidos

df_unido_fin['INTERNO_ITEM'].dropna(inplace =True)

valores_unicos = df_unido_fin['INTERNO_ITEM'].dropna().unique()

valores_unicos

df_unido_fin['ANO_ITEM'] = '2025' #FECHA ELABORACION DEL PLANO REVISAR
df_unido_fin['INTERNO_ITEM'] = df_unido_fin['INTERNO_ITEM'].dropna()
#df_unido_fin['INTERNO_ITEM'] =df_unido_fin['INTERNO_ITEM'].astype(int)
# 1. Obtener valores únicos sin incluir NaN
valores_unicos = df_unido_fin['INTERNO_ITEM'].dropna().unique()

# 2. Crear DataFrame con esos valores
df_unicos = pd.DataFrame({
    'INTERNO_ITEM': valores_unicos
})

df_unicos = df_unicos.rename(columns=  {'INTERNO_ITEM':'ITEM'})

df_unicos['ITEM'].astype(int).inplace=True

df_unicos['ITEM']

df_unicos = df_unicos[df_unicos['ITEM'].isin(list(range(30, 99)) + [100])]

df_unicos

item_data

item_data.rename(columns={'ITEM_INTERNO': 'ITEM'},inplace=True)

try:
  item_data.drop(129, inplace=True)
except:
  print('no data')
df_unicos['ITEM'] = df_unicos['ITEM'].astype(int) #.str.strip()
item_data['ITEM'] = item_data['ITEM'].astype(int) #.str.strip()
# Realiza el merge
df_merged = df_unicos.merge(item_data, on='ITEM')  # Usa 'inner' si solo quieres coincidencias

df_ordered = df_merged.sort_values(by="ITEM", ascending=True)

df_ordered['UNIDAD'] = df_ordered['UNIDAD'].replace({
    'ML': 'METRO LINEAL',
    'M': 'METRO LINEAL',
    'M2': 'METRO CUADRADO',
    'UND': 'UNIDAD',
    'UN': 'UNIDAD'
})

def_dem_contrato = df_ordered.copy()

df_ordered

#del  df_sen_item['CANTIDAD']
df_ordered = df_ordered.rename(columns={'ITEM':'INTERNO_ITEM','DESCRIPCIÓN':'DESCRIPCION_ITEM','2025_PRECIO UNITARIO INCLUYE AIU': 'COSTOS','2025_CANTIDAD':'CANTIDAD'})
df_ordered['ANO_ITEM'] = 2025  #año de fecha elaboracion del plano

df_ordered = df_ordered[["INTERNO_ITEM","UNIDAD","DESCRIPCION_ITEM","ANO_ITEM","CANTIDAD","COSTOS"]]
#[["INTERNO_ITEM","UNIDAD","DESCRIPCION_ITEM","AÑO_ITEM","CANTIDAD","COSTO"]]

df_ordered = df_ordered.drop_duplicates()

df_dem_item = df_ordered.copy()

df_dem_item

"""# DEM_DEMAR_ITEMS"""

df_concatenado_dem_int

df_concatenado_dem_int

df_concatenado_dem_int

#df_concatenado_dem_int['ANO_ITEM'] = df_dem_item['ANO_ITEM'].iloc[0]
#del df_concatenado_item_int['CANTIDAD']

df_concatenado_dem_int = df_concatenado_dem_int.dropna(subset=['ITEM'])
df_concatenado_dem_int['INTERNO_DEMARCACION'] = df_concatenado_dem_int['INTERNO_DEMARCACION'].fillna(method='ffill')

#df_concatenado_item_int
df_dem_dem_i= df_concatenado_dem_int.merge(df_unido_fin, on='INTERNO_DEMARCACION', how='left')

#df_dem_dem_i = df_dem_dem_i.rename(columns={'ITEM':'INTERNO_ITEM'})
df_dem_dem_i= df_dem_dem_i[["INTERNO_DETALLE","INTERNO_DEMARCACION","CLASE_DEMARCACION","INTERNO_ITEM","ANO_ITEM"]]
#df_dem_dem_i = df_dem_dem_i.rename(columns={'ANO_ITEM_x':'ANO_ITEM'})

df_dem_dem_i = df_dem_dem_i.drop_duplicates()

df_dem_dem_i



"""# DEM_CONTRATO_ITEM

"""

#
df_dem_dem_item_costo = def_dem_contrato.copy()
df_dem_dem_item_costo = def_dem_contrato.rename(columns={ '2025_SUBTOTAL':'COSTO_DIRECTO','ITEM': 'INTERNO_ITEM'})

df_dem_dem_item_costo['ANO_ITEM'] = df_dem_dem_i['ANO_ITEM'].iloc[0]

#"NUMERO_CONTRATO","INTERNO_ITEM","ANO_ITEM","COSTO_DIRECTO"
df_dem_dem_item_costo["NUMERO_CONTRATO"]= df_raw["NUMERO_CONTRATO"].iloc[0]
df_dem_dem_item_costo = df_dem_dem_item_costo[["NUMERO_CONTRATO","INTERNO_ITEM","ANO_ITEM","COSTO_DIRECTO"]]

output_filename_dem = f'Demarcacion_Output_{table_2["INTERNO_DISENO"].iloc[0]}.xlsx'

with pd.ExcelWriter(output_filename_dem) as writer:
    df_dem_demarcacion.to_excel(writer, sheet_name='DEM_DEMARCACION', index=False)
    DEM_DEMARCACION_DETALLE.to_excel(writer, sheet_name='DEM_DEMARCACION_DETALLE', index=False)
    df_dem_demarcacion_segmento.to_excel(writer, sheet_name='dem_demarcacion_segmento', index=False)
    df_dem_demarcacion_proyecto.to_excel(writer, sheet_name='dem_demarcacion_proyecto', index=False)
    df_dem_item.to_excel(writer, sheet_name='DEM_ITEM', index=False)
    df_dem_dem_i.to_excel(writer, sheet_name='DEM_DEMAR_ITEMS', index=False)
    df_dem_dem_item_costo.to_excel(writer, sheet_name='DEM_CONTRATO_ITEM', index=False)
    df_duplicate_tramos.to_excel(writer, sheet_name='VALIDACION_DIRECCION', index=False)


print(f"✅ DataFrames exported to {output_filename_dem}")

excel_input_files = {}

# Check the 'file' variable
if 'file' in locals() and isinstance(file, str) and ('.xls' in file or '.xlsx' in file):
    excel_input_files['file'] = file

# Check the 'file_1' variable
if 'file_1' in locals() and isinstance(file_1, str) and ('.xls' in file_1 or '.xlsx' in file_1):
    excel_input_files['file_1'] = file_1

if excel_input_files:
    print("Identified Excel Input Files and Variables:")
    for var, file_name in excel_input_files.items():
        print(f"- Variable: {var}, File: {file_name}")
else:
    print("No Excel input files found using the specified variables.")

# Create a boolean mask for rows where the specified columns have duplicates within the row
mask_duplicates = data.apply(lambda row: len(set([row['TRAMO_EJE.1'], row['TRAMO_INICIO.1'], row['TRAMO_TERMINA.1']])) < 3, axis=1)

# Filter the DataFrame using the mask and display all columns
df_duplicate_tramos = data.loc[mask_duplicates, :].copy()

# Display the resulting DataFrame
#display(df_duplicate_tramos)

print(data.columns)
#display(data.head())

try:
  data = clean_multilevel_columns(dataset)
  if data.empty:
    print("❗ DataFrame 'data' está vacío. Por favor, cargue un nuevo dataset en la celda 2 para continuar.")
    raise SystemExit("DataFrame vacío, deteniendo ejecución.") # Stop execution if DataFrame is empty
  else:
    print("✅ DataFrame 'data' cargado correctamente:")
    display(data.head()) # Use display for better formatting
except Exception as e:
  print(f"⚠️ Error al cargar o procesar el archivo: {e}")
  print("Por favor, verifica el nombre del archivo o la hoja y vuelve a intentarlo.")
  raise SystemExit("Error al cargar datos, deteniendo ejecución.") # Stop execution on error